{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/terris-citizen/75318a737ac7de088464cf673abafbd2/pr-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64QYifsi4ulX"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUHfwsPFvT0b"
      },
      "outputs": [],
      "source": [
        "!pip install PyGithub pandas matplotlib ace-tools-open pytz tabulate pytest datetime timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHQQkw_5vT0f"
      },
      "source": [
        "# Git Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP16WwRFvT0f"
      },
      "outputs": [],
      "source": [
        "GITHUB_TOKEN = \" \" ## Secret!\n",
        "# Comma-delimited list\n",
        "REPO_NAMES = \"dev-guy/git-pr-analysis-jupyter\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da1hHV-evT0e"
      },
      "source": [
        "# Specify How Much History to Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYvQl4ttvT0e"
      },
      "outputs": [],
      "source": [
        "# 30 days of data takes about 5 minutes\n",
        "max_days = 45\n",
        "per_page = 50  # Max per request\n",
        "max_pages = 500 # Prevent infinite loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K8RhcLz4opb"
      },
      "outputs": [],
      "source": [
        "# Calculate the cutoff date\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "\n",
        "cutoff_date = datetime.now(pytz.UTC) - timedelta(days=max_days)\n",
        "\n",
        "print (f\"Processing {REPO_NAMES} for PRs after {cutoff_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIu2mnovT0d"
      },
      "source": [
        "# Function: Compute duration between dates, ignoring weekends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HESTjjJd6kIt"
      },
      "source": [
        "Prompt: Write two python functions. The first is duration_seconds(begin, end) which takes two dates and computes the number of seconds between them and subtracts all \"weekends\" that overlap with the provided range. There are five global variables used by this function: timezone, begin_weekday, begin_time, end_weekday, and end_time . begin_weekday and end_weekday are short strings representing weekdays, for example Friday 2:00 PM and Monday 8:00 AM. Note that a weekend occurs in every week and correctly writing this function probably requires iterating over each week between begin and end. These global variables represent the begin and end times in each week that define the \"weekend\".\n",
        "\n",
        "Write a test that calls the function multiple times and checks the return value. test at least the following cases. Add more cases that you think would be challenging.\n",
        "1. the date range spans two weekends\n",
        "2. the beginning of the date range falls in the middle of a weekend\n",
        "3. the end of the date range falls in the middle of a weekend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuQEXVYc-xPG"
      },
      "outputs": [],
      "source": [
        "# Weekend definition\n",
        "timezone = pytz.timezone('America/Los_Angeles')\n",
        "\n",
        "WEEKEND_START_DAY = 4  # Friday (Mon=0, Tue=1, Wed=2, Thu=3, Fri=4, Sat=5, Sun=6)\n",
        "WEEKEND_START_HOUR = 14 # 2 PM\n",
        "WEEKEND_END_DAY = 0    # Monday\n",
        "WEEKEND_END_HOUR = 4   # 4 AM\n",
        "\n",
        "# The tests are based on the following:\n",
        "# timezone = pytz.timezone('America/Los_Angeles')\n",
        "# WEEKEND_START_DAY = 3  # Thursday (Mon=0, Tue=1, Wed=2, Thu=3, Fri=4, Sat=5, Sun=6)\n",
        "# WEEKEND_START_HOUR = 18\n",
        "# WEEKEND_END_DAY = 0    # Monday\n",
        "# WEEKEND_END_HOUR = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JbH0i8k6iNN"
      },
      "outputs": [],
      "source": [
        "def duration_seconds(begin: datetime, end: datetime) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the number of seconds between two dates, excluding all time in\n",
        "    the repeating \"weekend window\":\n",
        "    \"\"\"\n",
        "    total_seconds = (end - begin).total_seconds()\n",
        "    if total_seconds <= 0:\n",
        "        return 0\n",
        "\n",
        "    # 1) We'll sum the weekend overlap in `weekend_seconds`.\n",
        "    weekend_seconds = 0\n",
        "\n",
        "    # 2) Find the \"Monday 00:00\" of the week that includes `begin`.\n",
        "    #    (Because Monday is weekday=0; we subtract `begin.weekday()` days from `begin`.)\n",
        "    monday_of_current_week = datetime(\n",
        "        year=begin.year,\n",
        "        month=begin.month,\n",
        "        day=begin.day,\n",
        "        hour=0,\n",
        "        minute=0,\n",
        "        second=0,\n",
        "        tzinfo=begin.tzinfo\n",
        "    ) - timedelta(days=begin.weekday())\n",
        "\n",
        "    # 3) We'll iterate weekly from that Monday until we've covered all weekends\n",
        "    #    that might overlap [begin, end].\n",
        "    current_monday = monday_of_current_week\n",
        "\n",
        "    while True:\n",
        "        # Weekend start in the current week (Thursday 18:00).\n",
        "        # current_monday is Monday 00:00, so add 3 days (Thu) + set hour=18.\n",
        "        w_start = current_monday + timedelta(days=WEEKEND_START_DAY)\n",
        "        w_start = w_start.replace(hour=WEEKEND_START_HOUR, minute=0, second=0)\n",
        "\n",
        "        # Weekend end in the current week (the next Monday 08:00).\n",
        "        # current_monday + 7 days => next Monday 00:00, then hour=8.\n",
        "        w_end = current_monday + timedelta(days=7 + WEEKEND_END_DAY)\n",
        "        w_end = w_end.replace(hour=WEEKEND_END_HOUR, minute=0, second=0)\n",
        "\n",
        "        # If the weekend starts after `end`, no more weekends to process\n",
        "        if w_start >= end:\n",
        "            break\n",
        "\n",
        "        # If this entire weekend block is before `begin`, move to the next week\n",
        "        if w_end <= begin:\n",
        "            current_monday += timedelta(days=7)\n",
        "            continue\n",
        "\n",
        "        # Compute overlap between [w_start, w_end] and [begin, end].\n",
        "        overlap_start = max(w_start, begin)\n",
        "        overlap_end = min(w_end, end)\n",
        "\n",
        "        if overlap_end > overlap_start:\n",
        "            weekend_seconds += (overlap_end - overlap_start).total_seconds()\n",
        "\n",
        "        # If this weekend end goes beyond our `end`, we're done.\n",
        "        if w_end >= end:\n",
        "            break\n",
        "\n",
        "        # Otherwise, proceed to the next week's Monday\n",
        "        current_monday += timedelta(days=7)\n",
        "\n",
        "    # Subtract total weekend overlap\n",
        "    net_seconds = total_seconds - weekend_seconds\n",
        "    return max(int(net_seconds), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjF2p8q--iVZ"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "\n",
        "# If needed, import or define the duration_seconds function here:\n",
        "# from your_module import duration_seconds\n",
        "\n",
        "def test_duration_seconds():\n",
        "    # 1) Date range spans two weekends\n",
        "    # Weekend is from Thursday 18:00 -> Monday 08:00.\n",
        "    # Example range: Monday 2025-02-03 09:00 -> Tuesday 2025-02-18 10:00\n",
        "    # This crosses these weekends:\n",
        "    #   - Thu 2025-02-06 18:00 -> Mon 2025-02-10 08:00\n",
        "    #   - Thu 2025-02-13 18:00 -> Mon 2025-02-17 08:00\n",
        "    #\n",
        "    # Total time: 15 days + 1 hour = 361 hours = 361 * 3600 = 1299600 seconds\n",
        "    # Each weekend = 86 hours = 309600 seconds\n",
        "    # Two weekends = 619200 seconds\n",
        "    # 1299600 - 619200 = 680400\n",
        "    begin = datetime(2025, 2, 3, 9, 0)\n",
        "    end = datetime(2025, 2, 18, 10, 0)\n",
        "    expected = 680400\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 1: expected {expected}, got {result}\"\n",
        "\n",
        "    # 2) Beginning of the date range falls in the middle of a weekend\n",
        "    # Weekend: Thu 2025-02-06 18:00 -> Mon 2025-02-10 08:00\n",
        "    # Example: Friday 2025-02-07 12:00 -> Monday 2025-02-10 10:00\n",
        "    # From Fri 12:00 -> Mon 08:00 is entirely weekend, so no work time.\n",
        "    # Only Mon 08:00 -> Mon 10:00 = 2 hours = 7200 seconds of working time.\n",
        "    begin = datetime(2025, 2, 7, 12, 0)  # Friday noon\n",
        "    end = datetime(2025, 2, 10, 10, 0)   # Monday 10:00 AM\n",
        "    expected = 7200\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 2: expected {expected}, got {result}\"\n",
        "\n",
        "    # 3) End of the date range falls in the middle of a weekend\n",
        "    # Weekend: Thu 2025-02-06 18:00 -> Mon 2025-02-10 08:00\n",
        "    # Example: Monday 2025-02-03 09:00 -> Friday 2025-02-07 20:00\n",
        "    # Working time is Mon 09:00 -> Thu 18:00.\n",
        "    # Thu 18:00 -> Fri 20:00 is weekend, so excluded.\n",
        "    #\n",
        "    # Mon 09:00 -> Mon midnight = 15 hours\n",
        "    # Tue full day = 24 hours\n",
        "    # Wed full day = 24 hours\n",
        "    # Thu 00:00 -> Thu 18:00 = 18 hours\n",
        "    # Total = (15 + 24 + 24 + 18) hours = 81 hours = 291600 seconds\n",
        "    begin = datetime(2025, 2, 3, 9, 0)   # Monday 9:00 AM\n",
        "    end = datetime(2025, 2, 7, 20, 0)    # Friday 8:00 PM\n",
        "    expected = 291600\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 3: expected {expected}, got {result}\"\n",
        "\n",
        "    # 4) No crossing of weekend\n",
        "    # For example, from Tuesday 2025-02-04 10:00 -> Wednesday 2025-02-05 10:00\n",
        "    # That's exactly 24 hours of working time.\n",
        "    expected = 24 * 3600  # 86400\n",
        "    begin = datetime(2025, 2, 4, 10, 0)\n",
        "    end = datetime(2025, 2, 5, 10, 0)\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 4: expected {expected}, got {result}\"\n",
        "\n",
        "    # 5) Entirely within weekend\n",
        "    # Weekend is Thu 18:00 -> Mon 08:00\n",
        "    # Example: Friday 2025-02-07 10:00 -> Friday 2025-02-07 17:00\n",
        "    # This is entirely during the weekend; expected = 0\n",
        "    begin = datetime(2025, 2, 7, 10, 0)\n",
        "    end = datetime(2025, 2, 7, 17, 0)\n",
        "    expected = 0\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 5: expected {expected}, got {result}\"\n",
        "\n",
        "    # 6) End is before begin (should return 0)\n",
        "    begin = datetime(2025, 2, 5, 10, 0)\n",
        "    end = datetime(2025, 2, 5, 9, 0)\n",
        "    expected = 0\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 6: expected {expected}, got {result}\"\n",
        "\n",
        "    # 7) Tuesday midnight to Tuesday 2 AM - should not overlap a weekend\n",
        "    #    So we expect full 2 hours = 2*3600 = 7200 seconds\n",
        "    begin = datetime(2025, 2, 4, 0, 0)  # Tuesday 00:00\n",
        "    end = datetime(2025, 2, 4, 2, 0)   # Tuesday 02:00\n",
        "    expected = 7200\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 7: expected {expected}, got {result}\"\n",
        "\n",
        "    # 8) Starts in a weekend and ends in a weekend\n",
        "    # Weekend = Thursday 18:00 -> Monday 08:00.\n",
        "    #\n",
        "    # Example: Friday 2025-02-07 10:00 -> Sunday 2025-02-09 22:00.\n",
        "    # Both begin and end are within the same weekend block:\n",
        "    #   Thu (2/6) 18:00 -> Mon (2/10) 08:00\n",
        "    #\n",
        "    # Hence the entire interval is weekend — expect 0 seconds of working time.\n",
        "    begin = datetime(2025, 2, 7, 10, 0)  # Friday 10:00\n",
        "    end = datetime(2025, 2, 9, 22, 0)    # Sunday 22:00\n",
        "    expected = 0\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 8: expected {expected}, got {result}\"\n",
        "\n",
        "    # 9) Begin is one hour before the weekend, end is in the weekend\n",
        "    # Weekend (Thursday 18:00 -> Monday 08:00)\n",
        "    # Example: Thursday (2025-02-06) 17:00 -> Friday (2025-02-07) 10:00\n",
        "    # Working time is only Thu 17:00 -> Thu 18:00 = 1 hour = 3600 seconds\n",
        "    begin = datetime(2025, 2, 6, 17, 0)  # Thursday 17:00\n",
        "    end = datetime(2025, 2, 7, 10, 0)    # Friday 10:00\n",
        "    expected = 3600  # Only 1 hour of work time\n",
        "    result = duration_seconds(begin, end)\n",
        "    assert result == expected, f\"Case 9: expected {expected}, got {result}\"\n",
        "\n",
        "# Do not run the tests! The need specific global variables.\n",
        "# test_duration_seconds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV9ji5jgvT0f"
      },
      "source": [
        "# Function: Download PRs for One Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3p_Gir_vT0f"
      },
      "outputs": [],
      "source": [
        "processed_all_prs = True\n",
        "\n",
        "def get_prs(repo):\n",
        "    page = 0  # Start at the first page\n",
        "    processed_all_for_repo = False\n",
        "    repo_prs = []\n",
        "\n",
        "    while page < max_pages:\n",
        "        prs = repo.get_pulls(\n",
        "            sort=\"created\",\n",
        "            direction=\"desc\",\n",
        "            state=\"all\"\n",
        "        )[page * per_page : (page + 1) * per_page]  # Manual pagination\n",
        "\n",
        "        # Filter PRs by created_at date\n",
        "        filtered_prs = [pr for pr in prs if pr.created_at >= cutoff_date]\n",
        "\n",
        "        if not filtered_prs:\n",
        "            processed_all_for_repo = True\n",
        "            break  # No more PRs in the date range, exit loop\n",
        "\n",
        "        repo_prs.extend(filtered_prs)\n",
        "        page += 1  # Move to next page\n",
        "\n",
        "    if not processed_all_for_repo:\n",
        "        print(f\"Loop terminated early due to max_pages ({max_pages}) failsafe\")\n",
        "        processed_all_prs = False\n",
        "\n",
        "    return repo_prs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im1NEBdEvT0f"
      },
      "source": [
        "# Function: Process PR Reviews for One Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6WFweO3vT0g"
      },
      "outputs": [],
      "source": [
        "pr_data = []\n",
        "\n",
        "# Reviewers is a map from user name to { repo/PR/date }\n",
        "reviewers = {}\n",
        "\n",
        "now = datetime.now(pytz.utc)\n",
        "\n",
        "def get_pr_data(repo, prs):\n",
        "    for pr in prs:\n",
        "        if pr.user.login == \"dependabot[bot]\":\n",
        "            continue\n",
        "\n",
        "        first_review_at = None\n",
        "        ready_for_review_at = None\n",
        "        time_to_merge = None\n",
        "        time_to_first_review = None\n",
        "        open_duration = None\n",
        "        reviews = None\n",
        "\n",
        "        if pr.draft:\n",
        "            if pr.closed_at is not None:\n",
        "                continue\n",
        "        else:\n",
        "            # When was the PR ready for review?\n",
        "            issue = repo.get_issue(pr.number)\n",
        "            timeline = issue.get_timeline()\n",
        "            ready_for_review_at = pr.created_at\n",
        "\n",
        "            for event in timeline:\n",
        "                if event.event == \"ready_for_review\":\n",
        "                    ready_for_review_at = event.created_at\n",
        "                    break\n",
        "\n",
        "            reviews = pr.get_reviews()\n",
        "            # remove reviews by the author and CodeRabbit\n",
        "            reviews = [\n",
        "                review for review in reviews\n",
        "                if review.user.login != pr.user.login and\n",
        "                    review.user.login != \"coderabbitai[bot]\"\n",
        "            ]\n",
        "\n",
        "            if len(reviews) == 0:\n",
        "                if pr.closed_at is not None:\n",
        "                    continue\n",
        "            else:\n",
        "                # reviewed_by is a map of user name and review\n",
        "                reviewed_by = {}\n",
        "                for review in reviews:\n",
        "                    if review.user.login not in reviewed_by:\n",
        "                        reviewed_by[review.user.login] = review\n",
        "\n",
        "                # For each item in submitted_by, add the item to reviewers\n",
        "                for user, review in reviewed_by.items():\n",
        "                    if user not in reviewers:\n",
        "                        reviewers[user] = []\n",
        "                    reviewers[user].append({\n",
        "                        \"Repository\": pr.base.repo.full_name,\n",
        "                        \"PR\": pr.number,\n",
        "                        \"Submitted At\": review.submitted_at,\n",
        "                        \"Submitted By\": user,\n",
        "                    })\n",
        "\n",
        "                first_review_at = reviews[0].submitted_at\n",
        "                if first_review_at < ready_for_review_at:\n",
        "                    # First review occurred before marked as not a draft\n",
        "                    time_to_first_review = duration_seconds(pr.created_at, first_review_at) / 3600\n",
        "                else:\n",
        "                    time_to_first_review = duration_seconds(ready_for_review_at, first_review_at) / 3600\n",
        "\n",
        "            if pr.closed_at is None:\n",
        "                open_duration = duration_seconds(ready_for_review_at, now) / 3600\n",
        "\n",
        "            if pr.merged_at is not None:\n",
        "                time_to_merge = duration_seconds(pr.created_at, pr.merged_at) / 3600\n",
        "\n",
        "        pr_data.append({\n",
        "            \"Repository\": pr.base.repo.full_name,\n",
        "            \"PR\": pr.number,\n",
        "            \"Author\": pr.user.login,\n",
        "            \"Created At\": pr.created_at,\n",
        "            \"Ready For Review At\": ready_for_review_at,\n",
        "            \"First Review At\": first_review_at,\n",
        "            \"Time to First Review (hrs)\": time_to_first_review,\n",
        "            \"Merged At\": pr.merged_at,\n",
        "            \"Time to Merge (hrs)\": time_to_merge,\n",
        "            \"Number of Reviews\": len(reviews) if reviews is not None else None,\n",
        "            \"Waiting for First Review (hrs)\": open_duration if reviews is not None and len(reviews) == 0 else None,\n",
        "            \"Is Draft\": pr.draft,\n",
        "            \"Closed At\": pr.closed_at,\n",
        "            \"Open (hrs)\": open_duration,\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6o5gR_7_IzN"
      },
      "source": [
        "# Download PRs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbWnoiyX2itr"
      },
      "outputs": [],
      "source": [
        "from github import Github\n",
        "g = Github(GITHUB_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "acPGBoMY7U__"
      },
      "outputs": [],
      "source": [
        "for repo_name in REPO_NAMES.split(','):\n",
        "    repo_name2 = repo_name.strip()\n",
        "    repo = g.get_repo(repo_name2)\n",
        "    print (f\"Collecting PRs for {repo_name2}\")\n",
        "    prs = get_prs(repo)\n",
        "    print (f\"Processing PRs for {repo_name2}\")\n",
        "    get_pr_data(repo, prs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb8COXDRvT0g"
      },
      "source": [
        "# Begin Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jOrbD6xm7fwp"
      },
      "outputs": [],
      "source": [
        "if len(pr_data) == 0:\n",
        "    raise Exception(\"No PRs found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ERZrIpA0vT0g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Basic statistics\n",
        "\n",
        "min_created_at = min(pr[\"Created At\"] for pr in pr_data)\n",
        "max_created_at = max(pr[\"Created At\"] for pr in pr_data)\n",
        "min_merged_at = min((pr[\"Merged At\"] for pr in pr_data if pr[\"Merged At\"] is not None), default=None)\n",
        "max_merged_at = max((pr[\"Merged At\"] for pr in pr_data if pr[\"Merged At\"] is not None), default=None)\n",
        "\n",
        "num_prs = len(pr_data)\n",
        "num_drafts = sum(1 for pr in pr_data if pr[\"Is Draft\"])\n",
        "num_open = sum(1 for pr in pr_data if pr[\"Open (hrs)\"] is not None)\n",
        "num_waiting = sum(1 for pr in pr_data if pr[\"Waiting for First Review (hrs)\"] is not None)\n",
        "\n",
        "# Avg number of reviews per PR that has reviews\n",
        "valid_reviews = [\n",
        "    pr[\"Number of Reviews\"]\n",
        "    for pr in pr_data\n",
        "    if pr[\"Number of Reviews\"] is not None and pr[\"Number of Reviews\"] > 0\n",
        "]\n",
        "\n",
        "if valid_reviews:\n",
        "    avg_num_reviews = sum(valid_reviews) / len(valid_reviews)\n",
        "else:\n",
        "    avg_num_reviews = 0\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Metric\": [\n",
        "        \"Cutoff Date\",\n",
        "        \"Processed All PRs\",\n",
        "        \"PRs\",\n",
        "        \"Drafts\",\n",
        "        \"Open\",\n",
        "        \"Waiting for First Review\",\n",
        "        \"Avg Reviews per PR\",\n",
        "        \"Min Created At\",\n",
        "        \"Max Created At\",\n",
        "        \"Min Merged At\",\n",
        "        \"Max Merged At\",\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        cutoff_date,\n",
        "        processed_all_prs,\n",
        "        num_prs,\n",
        "        num_drafts,\n",
        "        num_open,\n",
        "        num_waiting,\n",
        "        avg_num_reviews,\n",
        "        min_created_at,\n",
        "        max_created_at,\n",
        "        min_merged_at,\n",
        "        max_merged_at,\n",
        "    ]})\n",
        "\n",
        "# Display the summary table\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fR_awW0fvT0g"
      },
      "outputs": [],
      "source": [
        "# Raw PR data\n",
        "import ace_tools_open as tools\n",
        "\n",
        "df = pd.DataFrame(pr_data)\n",
        "tools.display_dataframe_to_user(name=\"PRs\", dataframe=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVYaXja6f8tV"
      },
      "outputs": [],
      "source": [
        "# Reviews by Week\n",
        "reviewer_data = []\n",
        "\n",
        "for user, reviews in reviewers.items():\n",
        "    # Group reviews by week\n",
        "    week_counts = {}\n",
        "\n",
        "    for review in reviews:\n",
        "        # Parse the submission date\n",
        "        # submitted_at = datetime.fromisoformat(review[\"Submitted At\"].replace(\"Z\", \"+00:00\"))\n",
        "        submitted_at = review[\"Submitted At\"]\n",
        "\n",
        "        # Calculate the beginning of the week (Monday)\n",
        "        week_begin = submitted_at - timedelta(days=submitted_at.weekday())\n",
        "        week_begin = week_begin.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        # Calculate the end of the week (Sunday)\n",
        "        week_end = week_begin + timedelta(days=6, hours=23, minutes=59, seconds=59)\n",
        "\n",
        "        # Format dates for the key\n",
        "        week_key = (week_begin.date(), week_end.date())\n",
        "\n",
        "        # Count reviews per week\n",
        "        if week_key in week_counts:\n",
        "            week_counts[week_key] += 1\n",
        "        else:\n",
        "            week_counts[week_key] = 1\n",
        "\n",
        "    # Add data to our table\n",
        "    for (week_begin, week_end), count in week_counts.items():\n",
        "        reviewer_data.append({\n",
        "            \"Week\": 0,\n",
        "            \"Week Begin\": week_begin,\n",
        "            \"Week End\": week_end,\n",
        "            \"Reviewer\": user,\n",
        "            \"PRs\": count,\n",
        "        })\n",
        "\n",
        "df_reviewers = pd.DataFrame(reviewer_data)\n",
        "\n",
        "# Create a mapping of unique weeks to week numbers (0, -1, -2, etc.)\n",
        "unique_weeks = df_reviewers[\"Week Begin\"].unique()\n",
        "week_to_number = {week: -i for i, week in enumerate(unique_weeks)}\n",
        "\n",
        "# Add the week number column\n",
        "df_reviewers[\"Week\"] = df_reviewers[\"Week Begin\"].map(week_to_number)\n",
        "\n",
        "# Sort by user and week begin date\n",
        "df_reviewers = df_reviewers.sort_values(by=[\"Week\", \"PRs\", \"Reviewer\"],ascending=[False, False, True])\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Reviews By Week\", dataframe=df_reviewers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d0cxfguYvT0g"
      },
      "outputs": [],
      "source": [
        "# Time metrics\n",
        "def analyze_time_ranges(series, metric_name):\n",
        "    total = len(series.dropna())\n",
        "\n",
        "    # Calculate counts and percentages for each range\n",
        "    ranges = {\n",
        "        \"< 1 day\": (series <= 24).sum(),\n",
        "        \"1-2 days\": ((series > 24) & (series <= 48)).sum(),\n",
        "        \"2-3 days\": ((series > 48) & (series <= 72)).sum(),\n",
        "        \"3-4 days\": ((series > 72) & (series <= 96)).sum(),\n",
        "        \"> 4 days\": (series > 96).sum()\n",
        "    }\n",
        "\n",
        "    # Convert to percentages\n",
        "    percentages = {k: (v/total * 100) for k, v in ranges.items()}\n",
        "\n",
        "    print(f\"\\n{metric_name} Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "    for range_name, count in ranges.items():\n",
        "        print(f\"{range_name}: {count} PRs ({percentages[range_name]:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vLcbaMLpL6f2"
      },
      "outputs": [],
      "source": [
        "# Time metrics\n",
        "analyze_time_ranges(df[\"Open (hrs)\"], \"Open Time\")\n",
        "analyze_time_ranges(df[\"Waiting for First Review (hrs)\"], \"Waiting for First Review\")\n",
        "analyze_time_ranges(df[\"Time to First Review (hrs)\"], \"Time to First Review\")\n",
        "analyze_time_ranges(df[\"Time to Merge (hrs)\"], \"Time to Merge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qyWeVRmkvT0g"
      },
      "outputs": [],
      "source": [
        "# Time to first review and merge distribution charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_time_ranges(df, metric_col, title):\n",
        "    total = len(df[metric_col].dropna())\n",
        "\n",
        "    ranges = {\n",
        "        \"< 1 day\": (df[metric_col] <= 24).sum() / total * 100,\n",
        "        \"1-2 days\": ((df[metric_col] > 24) & (df[metric_col] <= 48)).sum() / total * 100,\n",
        "        \"2-3 days\": ((df[metric_col] > 48) & (df[metric_col] <= 72)).sum() / total * 100,\n",
        "        \"3-4 days\": ((df[metric_col] > 72) & (df[metric_col] <= 96)).sum() / total * 100,\n",
        "        \"> 4 days\": (df[metric_col] > 96).sum() / total * 100\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(ranges.keys(), ranges.values())\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Percentage of PRs\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w55ZavsZMlDh"
      },
      "outputs": [],
      "source": [
        "# Plot time metrics\n",
        "plot_time_ranges(df, \"Open (hrs)\", \"Open Time Distribution\")\n",
        "plot_time_ranges(df, \"Waiting for First Review (hrs)\", \"Waiting for First Review Time Distribution\")\n",
        "plot_time_ranges(df, \"Time to First Review (hrs)\", \"Time to First Review Distribution\")\n",
        "plot_time_ranges(df, \"Time to Merge (hrs)\", \"Time to Merge Distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RhTk9hduER_4"
      },
      "outputs": [],
      "source": [
        "# Metrics by author\n",
        "author_metrics = df.groupby('Author').agg(\n",
        "    Total=('PR', 'count'),\n",
        "    Merged=('Merged At', lambda x: x.notna().sum()),\n",
        "    Drafts=('Is Draft', 'sum'),\n",
        "    Open=('Open (hrs)', lambda x: x.notna().sum()),\n",
        "    Waiting_For_Review=('Number of Reviews', lambda x: (x == 0).sum()),\n",
        "    Max_Waiting_For_First_Review_Hrs=('Waiting for First Review (hrs)', 'max'),\n",
        "    Median_Open_Hrs=('Open (hrs)', 'median')\n",
        ").sort_values('Total', ascending=False)\n",
        "\n",
        "author_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_mOVo0UCvT0h"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics for time to first review by author\n",
        "first_review_stats = df.groupby('Author').agg({\n",
        "    'Time to First Review (hrs)': ['mean', 'median', 'max']\n",
        "}).sort_values(('Time to First Review (hrs)', 'max'), ascending=False)\n",
        "\n",
        "# Calculate metrics for waiting for first review time by author\n",
        "waiting_for_review_stats = df.groupby('Author').agg({\n",
        "    'Waiting for First Review (hrs)': ['mean', 'median', 'max']\n",
        "}).sort_values(('Waiting for First Review (hrs)', 'max'), ascending=False)\n",
        "\n",
        "# Calculate metrics for open time by author\n",
        "open_stats = df.groupby('Author').agg({\n",
        "    'Open (hrs)': ['mean', 'median', 'max']\n",
        "}).sort_values(('Open (hrs)', 'max'), ascending=False)\n",
        "\n",
        "# Calculate metrics for time to merge by author\n",
        "merge_stats = df.groupby('Author').agg({\n",
        "    'Time to Merge (hrs)': ['mean', 'median', 'max']\n",
        "}).sort_values(('Time to Merge (hrs)', 'max'), ascending=False)\n",
        "\n",
        "# Format the tables\n",
        "first_review_stats.columns = ['Avg Time to First Review (hrs)', 'Median Time to First Review (hrs)', 'Max Time to First Review (hrs)']\n",
        "waiting_for_review_stats.columns = ['Avg Waiting for Review (hrs)', 'Median Waiting for Review (hrs)', 'Max Waiting for Review (hrs)']\n",
        "merge_stats.columns = ['Avg Time to Merge (hrs)', 'Median Time to Merge (hrs)', 'Max Time to Merge (hrs)']\n",
        "open_stats.columns = ['Avg Open (hrs)', 'Median Open (hrs)', 'Max Open (hrs)']\n",
        "\n",
        "print(first_review_stats.round(2))\n",
        "print(waiting_for_review_stats.round(2))\n",
        "print(merge_stats.round(2))\n",
        "print(open_stats.round(2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
